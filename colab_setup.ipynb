{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "setup_header"
      },
      "source": [
       "# 📚 Books Classification - Google Colab GPU Training\n",
       "\n",
       "This notebook trains a book sentence classification model using GPU acceleration on Google Colab.\n",
       "\n",
       "**Selected Books:**\n",
       "- Anna Karenina (Classics)\n",
       "- The Adventures of Alice in Wonderland (Children's Books)\n",
       "- Frankenstein (Science-Fiction)\n",
       "- The Life of Julius Caesar (Biographies)\n",
       "\n",
       "**Model:** Constructive Learning Model with BERT encoder"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "gpu_check"
      },
      "source": [
       "## 🔍 GPU Check\n",
       "\n",
       "First, let's verify we have GPU access:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "gpu_verification"
      },
      "outputs": [],
      "source": [
       "import torch\n",
       "print(f\"PyTorch version: {torch.__version__}\")\n",
       "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
       "if torch.cuda.is_available():\n",
       "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
       "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
       "    print(f\"GPU Memory Available: {torch.cuda.memory_allocated(0) / 1e9:.1f} GB used\")\n",
       "else:\n",
       "    print(\"⚠️  No GPU detected! Please enable GPU in Runtime > Change runtime type\")\n",
       "    print(\"   Runtime > Change runtime type > Hardware accelerator: GPU\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "install_deps"
      },
      "source": [
       "## 📦 Install Dependencies\n",
       "\n",
       "Install required packages for GPU training:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "install_packages"
      },
      "outputs": [],
      "source": [
       "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
       "!pip install transformers datasets tokenizers\n",
       "!pip install nltk scikit-learn pandas numpy matplotlib seaborn\n",
       "!pip install wandb tensorboard tqdm PyYAML\n",
       "!pip install accelerate deepspeed\n",
       "\n",
       "print(\"✅ All packages installed successfully!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "download_nltk"
      },
      "source": [
       "## 📥 Download NLTK Data\n",
       "\n",
       "Download required NLTK data for sentence tokenization:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "nltk_download"
      },
      "outputs": [],
      "source": [
       "import nltk\n",
       "nltk.download('punkt')\n",
       "nltk.download('punkt_tab')\n",
       "print(\"✅ NLTK data downloaded successfully!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "upload_files"
      },
      "source": [
       "## 📤 Upload Project Files\n",
       "\n",
       "Upload your project files to Colab. You can either:\n",
       "1. Upload the `books_classification_colab.zip` file we created\n",
       "2. Or upload individual files\n",
       "\n",
       "**Option 1: Upload ZIP file (Recommended)**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "file_upload_zip"
      },
      "outputs": [],
      "source": [
       "from google.colab import files\n",
       "import zipfile\n",
       "import os\n",
       "\n",
       "print(\"📤 Please upload the books_classification_colab.zip file:\")\n",
       "uploaded = files.upload()\n",
       "\n",
       "# Extract the uploaded file\n",
       "for filename in uploaded.keys():\n",
       "    if filename.endswith('.zip'):\n",
       "        print(f\"📦 Extracting {filename}...\")\n",
       "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
       "            zip_ref.extractall('.')\n",
       "        print(f\"✅ Extracted {filename}\")\n",
       "        break\n",
       "    else:\n",
       "        print(f\"⚠️  {filename} is not a ZIP file. Please upload books_classification_colab.zip\")\n",
       "\n",
       "print(\"📁 Files uploaded and extracted!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "verify_files"
      },
      "source": [
       "## ✅ Verify Project Files\n",
       "\n",
       "Let's verify that all necessary files are present:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "check_files"
      },
      "outputs": [],
      "source": [
       "import os\n",
       "\n",
       "required_files = [\n",
       "    'configs/config.yaml',\n",
       "    'data/prepare_data.py',\n",
       "    'models/constructive_model.py',\n",
       "    'train_cloud.py',\n",
       "    'requirements-cloud.txt'\n",
       "]\n",
       "\n",
       "print(\"🔍 Checking required files:\")\n",
       "all_present = True\n",
       "for file_path in required_files:\n",
       "    if os.path.exists(file_path):\n",
       "        print(f\"✅ {file_path}\")\n",
       "    else:\n",
       "        print(f\"❌ {file_path} - MISSING\")\n",
       "        all_present = False\n",
       "\n",
       "if all_present:\n",
       "    print(\"\\n🎉 All files present! Ready to proceed.\")\n",
       "else:\n",
       "    print(\"\\n⚠️  Some files are missing. Please check the upload.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "prepare_data"
      },
      "source": [
       "## 🗂️ Prepare Data\n",
       "\n",
       "Run data preparation to download and process the books dataset:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "run_data_prep"
      },
      "outputs": [],
      "source": [
       "import sys\n",
       "sys.path.append('.')\n",
       "\n",
       "print(\"📚 Starting data preparation...\")\n",
       "!python data/prepare_data.py\n",
       "print(\"✅ Data preparation completed!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "verify_dataset"
      },
      "source": [
       "## 📊 Verify Dataset\n",
       "\n",
       "Let's check the prepared dataset:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "check_dataset"
      },
      "outputs": [],
      "source": [
       "from datasets import load_from_disk\n",
       "import json\n",
       "\n",
       "# Load dataset\n",
       "dataset = load_from_disk('data/processed_dataset')\n",
       "print(f\"📊 Dataset loaded successfully!\")\n",
       "print(f\"   Train: {len(dataset['train'])} samples\")\n",
       "print(f\"   Validation: {len(dataset['validation'])} samples\")\n",
       "print(f\"   Test: {len(dataset['test'])} samples\")\n",
       "\n",
       "# Load metadata\n",
       "with open('data/metadata.json', 'r') as f:\n",
       "    metadata = json.load(f)\n",
       "\n",
       "print(f\"\\n📚 Books in dataset:\")\n",
       "for book_id, book_name in metadata['id_to_label'].items():\n",
       "    print(f\"   {book_id}: {book_name}\")\n",
       "\n",
       "# Show sample\n",
       "print(f\"\\n📝 Sample sentence:\")\n",
       "print(f\"   {dataset['train'][0]['sentence'][:100]}...\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "setup_wandb"
      },
      "source": [
       "## 📊 Setup Weights & Biases (Optional)\n",
       "\n",
       "Setup experiment tracking for monitoring training progress:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "wandb_login"
      },
      "outputs": [],
      "source": [
       "import wandb\n",
       "\n",
       "# Uncomment the line below to login to WandB\n",
       "# wandb.login()\n",
       "\n",
       "print(\"📊 WandB setup completed!\")\n",
       "print(\"💡 To enable experiment tracking, uncomment 'wandb.login()' above\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "start_training"
      },
      "source": [
       "## 🚀 Start Training\n",
       "\n",
       "Start the cloud-optimized training with GPU acceleration:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "run_training"
      },
      "outputs": [],
      "source": [
       "print(\"🚀 Starting GPU training...\")\n",
       "print(f\"🎯 Training on: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
       "\n",
       "# Start training with 5 epochs for quick testing\n",
       "!python train_cloud.py --epochs 5\n",
       "\n",
       "print(\"✅ Training completed!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "check_results"
      },
      "source": [
       "## 📈 Check Training Results\n",
       "\n",
       "Let's check the training results and model performance:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "view_results"
      },
      "outputs": [],
      "source": [
       "import os\n",
       "import glob\n",
       "\n",
       "print(\"📊 Training Results:\")\n",
       "\n",
       "# Check for checkpoints\n",
       "checkpoints = glob.glob('experiments/checkpoints/*.pt')\n",
       "if checkpoints:\n",
       "    print(f\"✅ Found {len(checkpoints)} checkpoints:\")\n",
       "    for checkpoint in sorted(checkpoints):\n",
       "        size_mb = os.path.getsize(checkpoint) / 1024 / 1024\n",
       "        print(f\"   📁 {os.path.basename(checkpoint)} ({size_mb:.1f} MB)\")\n",
       "else:\n",
       "    print(\"⚠️  No checkpoints found\")\n",
       "\n",
       "# Check for logs\n",
       "logs = glob.glob('experiments/logs/*.log')\n",
       "if logs:\n",
       "    print(f\"\\n📝 Found {len(logs)} log files:\")\n",
       "    for log in logs:\n",
       "        print(f\"   📄 {os.path.basename(log)}\")\n",
       "\n",
       "print(\"\\n🎉 Training results ready!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "test_model"
      },
      "source": [
       "## 🧪 Test Model\n",
       "\n",
       "Test the trained model with sample sentences:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "model_test"
      },
      "outputs": [],
      "source": [
       "print(\"🧪 Testing trained model...\")\n",
       "!python test_prediction.py\n",
       "print(\"✅ Model testing completed!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "download_results"
      },
      "source": [
       "## 📥 Download Results\n",
       "\n",
       "Download the training results and model files:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "download_files"
      },
      "outputs": [],
      "source": [
       "import zipfile\n",
       "import os\n",
       "\n",
       "print(\"📦 Creating results package...\")\n",
       "\n",
       "# Create a zip file with results\n",
       "with zipfile.ZipFile('training_results.zip', 'w') as zipf:\n",
       "    # Add experiments directory\n",
       "    if os.path.exists('experiments'):\n",
       "        for root, dirs, files in os.walk('experiments'):\n",
       "            for file in files:\n",
       "                file_path = os.path.join(root, file)\n",
       "                zipf.write(file_path, file_path)\n",
       "    \n",
       "    # Add data directory\n",
       "    if os.path.exists('data'):\n",
       "        for root, dirs, files in os.walk('data'):\n",
       "            for file in files:\n",
       "                file_path = os.path.join(root, file)\n",
       "                zipf.write(file_path, file_path)\n",
       "\n",
       "# Download the results\n",
       "files.download('training_results.zip')\n",
       "print(\"✅ Results downloaded to your computer!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "next_steps"
      },
      "source": [
       "## 🎯 Next Steps\n",
       "\n",
       "### What you can do next:\n",
       "\n",
       "1. **📊 Analyze Results**: Check the training logs and metrics\n",
       "2. **🔧 Fine-tune**: Adjust hyperparameters in `configs/config.yaml`\n",
       "3. **🚀 Scale Up**: Train for more epochs or use larger models\n",
       "4. ",
       "**📱 Deploy**: Use the trained model for inference\n",
       "5. **📈 Monitor**: Set up WandB for better experiment tracking\n",
       "\n",
       "### Performance Tips:\n",
       "\n",
       "- **Free Colab**: Limited to ~12 hours, use for testing\n",
       "- **Colab Pro**: More hours, better GPUs (V100/A100)\n",
       "- **Batch Size**: Adjust based on GPU memory\n",
       "- **Checkpointing**: Saves progress every epoch\n",
       "\n",
       "### 🎉 Congratulations!\n",
       "\n",
       "You've successfully trained a book sentence classification model on Google Colab with GPU acceleration!"
      ]
     }
    ],
    "metadata": {
     "accelerator": "GPU",
     "colab": {
      "gpuType": "T4",
      "provenance": []
     },
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }