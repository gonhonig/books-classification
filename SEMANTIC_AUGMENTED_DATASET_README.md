# Semantic Augmented Dataset - Comprehensive Documentation

## 📋 Overview

This document provides comprehensive documentation for the **Semantic Augmented Dataset**, a multi-label classification dataset created using semantic similarity pairs. The dataset enables training models that can predict which books a sentence is semantically similar to, based on cross-book semantic relationships.

## 🎯 Dataset Purpose

The semantic augmented dataset addresses the limitations of traditional single-label classification by:
- **Leveraging semantic similarity** rather than just proximity in embedding space
- **Creating cross-book relationships** based on actual semantic content
- **Enabling multi-label classification** where sentences can belong to multiple books
- **Providing complete corpus coverage** with all sentences from the deduplicated corpus

## 📊 Dataset Statistics

### Basic Information
- **Total sentences**: 31,760
- **Unique sentences**: 31,667 (93 duplicates removed)
- **Books covered**: 4 (Anna Karenina, Frankenstein, The Adventures of Alice in Wonderland, Wuthering Heights)
- **Dataset format**: Multi-label classification
- **Split ratios**: Train=70%, Validation=15%, Test=15%

### Dataset Splits
- **Train**: 22,231 samples
- **Validation**: 4,764 samples
- **Test**: 4,765 samples

### Label Distribution
- **1 label**: 26,606 sentences (83.8%) - Original book only
- **2 labels**: 3,479 sentences (11.0%) - Original + 1 paired book
- **3 labels**: 1,264 sentences (4.0%) - Original + 2 paired books
- **4 labels**: 411 sentences (1.3%) - All books

## ⚖️ Balanced Dataset Sampling

### Sampling Strategy
The full semantic augmented dataset (31,760 sentences) is sampled to create a smaller, balanced dataset for model training:

1. **Multi-label Priority**: ALL multi-label sentences (5,154 sentences) are included
2. **Single-label Sampling**: For each book, additional single-label positive samples are added to reach target size
3. **Negative Sampling**: Negative samples are added to balance positive/negative ratios
4. **Target Size**: ~5,000 samples per class (balanced across books)

### Balanced Dataset Statistics
- **Total balanced samples**: 10,121 sentences
- **Multi-label samples**: 5,154 sentences (50.9%)
- **Single-label samples**: 4,967 sentences (49.1%)
- **Target samples per class**: 5,000
- **Balanced positive/negative ratios**: Yes

### Sampling Process Details
```python
# 1. Include ALL multi-label sentences
multi_label_indices = [i for i in range(len(df)) if np.sum(book_labels[i]) > 1]
selected_indices = multi_label_indices.copy()

# 2. Add single-label positive samples to reach target
for book_col in book_columns:
    current_positive = sum(1 for i in selected_indices if book_labels[i, book_idx] == 1)
    additional_needed = target_samples_per_class - current_positive
    # Sample additional single-label positive samples

# 3. Add negative samples to balance ratios
for book_col in book_columns:
    positive_count = sum(1 for i in selected_indices if book_labels[i, book_idx] == 1)
    negative_count = sum(1 for i in selected_indices if book_labels[i, book_idx] == 0)
    # Add negative samples if needed
```

## 🔧 Creation Process

### Step 1: Data Sources
- **Deduplicated Corpus**: `data/corpus_deduplicated.json` (31,760 sentences)
- **Semantic Pairs**: `data/semantic_pairs.json` (11,861 pairs)
- **Similarity Threshold**: 0.7 (minimum similarity for inclusion)

### Step 2: Dataset Generation
1. **Start with ALL sentences** from the deduplicated corpus
2. **Mark original book** for each sentence (1 label)
3. **Process semantic pairs** to add cross-book similarities
4. **Create multi-label annotations** (1-4 labels per sentence)

### Step 3: Dataset Structure
```python
{
    'sentence': 'Text content',
    'labels': [1, 0, 0, 1],  # Multi-label array for 4 books
    'original_label': 1,       # Single-label classification (0-3)
    'original_book': 'Anna Karenina'  # Source book name
}
```

## 📈 Analysis Results

### Cross-Book Similarity Analysis
1. **Anna Karenina ↔ Wuthering Heights**: 3,889 shared sentences (14.0% similarity)
2. **Anna Karenina ↔ The Adventures of Alice in Wonderland**: 1,593 shared sentences (6.8% similarity)
3. **Frankenstein ↔ Wuthering Heights**: 1,233 shared sentences (9.8% similarity)
4. **The Adventures of Alice in Wonderland ↔ Wuthering Heights**: 1,204 shared sentences (10.8% similarity)
5. **Anna Karenina ↔ Frankenstein**: 1,361 shared sentences (5.4% similarity)
6. **Frankenstein ↔ The Adventures of Alice in Wonderland**: 457 shared sentences (6.6% similarity)

### Key Insights
- **Anna Karenina and Wuthering Heights** show the strongest semantic similarity (14.0%)
- **Frankenstein and The Adventures of Alice in Wonderland** show the weakest similarity (6.6%)
- Most sentences have 1 label (original book only), with semantic pairs adding additional labels
- The dataset includes ALL sentences from the deduplicated corpus

### Examples of 4-Book Similarities
The most semantically universal sentences include:
- **"Oh dear!"** - Similar across all 4 books with variations like "Oh, dear!", "Eh, dear!", "And, oh dear!"
- **"Well!"** - Universal expression found as "Well, well!", "Oh, well!", "Ah, well!" across all books
- **"she said aloud"** - Similar dialogue patterns across all books
- **"Oh!"** - Universal exclamation with variations like "Ah!", "Aha!", "Oho!" across all books

These examples show how certain expressions, exclamations, and dialogue patterns are semantically similar across different literary works, regardless of genre or author.

## ✅ Dataset Splits Validation

### Distribution Consistency
All splits maintain very similar label distributions:
- **1 label**: Train=83.9%, Val=83.2%, Test=83.7% (✅ Consistent)
- **2 labels**: Train=10.9%, Val=11.2%, Test=11.2% (✅ Consistent)
- **3 labels**: Train=4.0%, Val=4.1%, Test=3.8% (✅ Consistent)
- **4 labels**: Train=1.3%, Val=1.5%, Test=1.2% (✅ Consistent)

### Book Distribution Across Splits
- **Anna Karenina**: ~70% across all splits (dominant book)
- **Wuthering Heights**: ~30% across all splits
- **Frankenstein**: ~14% across all splits
- **The Adventures of Alice in Wonderland**: ~9% across all splits

### Quality Assessment
1. **Distribution Consistency**: All label count distributions are within 1% difference across splits
2. **Proper Split Ratios**: 70/15/15 split maintained correctly
3. **Representative Sampling**: Each split accurately represents the overall dataset distribution
4. **No Data Leakage**: Random splitting preserved the natural distribution

## 📁 File Structure

### Core Dataset Files
```
data/
├── dataset/                           # Multi-label dataset splits
│   ├── train/                        # Training data
│   ├── validation/                    # Validation data
│   └── test/                         # Test data
├── semantic_augmented/                # Analysis and documentation
│   ├── semantic_augmented_dataset.csv
│   ├── semantic_augmented_dataset.json
│   ├── semantic_augmented_balanced_dataset.csv  # Balanced sampled dataset
│   ├── semantic_augmented_analysis.png
│   ├── semantic_augmented_analysis.json
│   ├── dataset_splits_analysis.png
│   ├── dataset_splits_analysis.json
│   ├── dataset_metadata.json
│   ├── dataset_metadata_summary.md
│   └── book_to_label_mapping.json
├── semantic_pairs.json               # Semantic pairs for augmentation
├── corpus_deduplicated.json          # Deduplicated corpus
├── metadata.json                     # Dataset metadata
└── embeddings_cache.npz              # Cached embeddings
```

### Scripts
- `create_augmented_dataset.py` - Main dataset creation script (includes balanced sampling)
- `analyze_semantic_augmented_dataset.py` - Dataset analysis script
- `analyze_dataset_splits.py` - Split distribution analysis script

## 🚀 Usage

### Loading the Dataset
```python
from datasets import load_from_disk

# Load the dataset
dataset = load_from_disk('data/dataset')

# Access splits
train_data = dataset['train']
val_data = dataset['validation']
test_data = dataset['test']

# Example data point
sample = train_data[0]
print(sample)
# Output: {
#     'sentence': 'Can t I live without him?',
#     'labels': [1, 0, 0, 0],
#     'original_label': 1,
#     'original_book': 'Anna Karenina'
# }
```

### Multi-Label Classification
```python
# Each sample has a 'labels' field with 4 binary values
# [Anna_Karenina, Frankenstein, Alice_in_Wonderland, Wuthering_Heights]
labels = sample['labels']  # [1, 0, 0, 0] means similar to Anna Karenina only
```

## 🎯 Advantages of This Approach

### 1. Semantic Understanding
- Leverages actual semantic similarity rather than just proximity
- Based on human-annotated semantic pairs
- Captures meaningful cross-book relationships

### 2. Complete Coverage
- Includes ALL sentences from the deduplicated corpus
- No sentences are lost during the augmentation process
- Maintains original corpus integrity

### 3. Multi-Label Learning
- Enables training models that can predict multiple book associations
- Realistic representation of semantic relationships
- Supports complex classification scenarios

### 4. Quality Control
- Uses similarity threshold (0.7) to ensure high-quality pairs
- Validates distribution consistency across splits
- Comprehensive analysis and documentation

### 5. Balanced Sampling Strategy
- **Multi-label Priority**: Preserves all semantically rich multi-label sentences
- **Controlled Size**: Creates manageable dataset (~10K samples) for efficient training
- **Balanced Classes**: Ensures equal representation across all books
- **Positive/Negative Balance**: Maintains balanced positive/negative ratios per book
- **Reproducible**: Uses fixed random seed for consistent sampling

## 📊 Comparison with Previous Approaches

### vs. Single-Label Classification
- **Before**: Each sentence belongs to exactly one book
- **After**: Each sentence can belong to 1-4 books based on semantic similarity
- **Benefit**: More realistic representation of semantic relationships

### vs. Original Balanced Dataset
- **Before**: 14,750 sentences with single labels
- **After**: 31,760 sentences with multi-labels
- **Benefit**: Larger dataset with richer semantic information

## 🔮 Potential Use Cases

1. **Multi-Label Classification**: Train models to predict which books a sentence is semantically similar to
2. **Cross-Book Analysis**: Study semantic relationships between different literary works
3. **Style Transfer**: Understand how similar concepts are expressed across different authors
4. **Semantic Search**: Find sentences that are semantically similar across books
5. **Literary Analysis**: Analyze patterns in cross-book semantic similarities

## 📈 Next Steps

1. **Model Training**: Use this dataset to train multi-label classification models
2. **Evaluation**: Compare performance with single-label classification approaches
3. **Feature Engineering**: Extract additional features from the semantic relationships
4. **Cross-Validation**: Implement proper cross-validation for multi-label learning

## 📝 Technical Notes

- **Similarity Threshold**: 0.7 (minimum similarity for inclusion)
- **Dataset Format**: HuggingFace DatasetDict with train/validation/test splits
- **Label Format**: Binary arrays [0,1,0,1] for 4 books
- **Metadata**: Comprehensive metadata including label distributions and book information
- **Validation**: Distribution consistency validated across all splits

## 🤝 Contributing

This dataset was created as part of a comprehensive study on semantic similarity and multi-label classification for literary texts. The methodology and insights can be applied to other domains requiring semantic understanding and multi-label classification.

---

**Created**: August 2024  
**Dataset Version**: 1.0  
**Status**: ✅ Complete and validated  
**Ready for**: Model training and evaluation 