# Project Configuration
project:
  name: "books-classification"
  version: "1.0.0"
  description: "Multi-label classification of English book sentences using semantic embeddings"

# Data Configuration (Step 1)
data:
  corpus_name: "english_books_corpus"
  selected_books:
    - "Anna Karenina"
    - "The Adventures of Alice in Wonderland"
    - "Frankenstein"
    - "Wuthering Heights"
  max_sentences_per_book: null  # Use all sentences
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  max_sentence_length: 512
  min_sentence_length: 10
  random_seed: 42
  
  # Data preprocessing
  preprocessing:
    remove_duplicates: true
    normalize_text: true
    remove_special_chars: false
    lowercase: false

# Semantic Model Selection (Step 2)
semantic_models:
  candidates:
    - "sentence-transformers/all-MiniLM-L6-v2"
    - "sentence-transformers/all-mpnet-base-v2"
    - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    - "sentence-transformers/paraphrase-MiniLM-L3-v2"
  
  # Selected best model from Step 2
  selected_model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  
  evaluation:
    similarity_threshold: 0.7
    test_pairs_file: "data/similarity_test_pairs.json"
    metrics: ["cosine_similarity", "semantic_similarity_score"]

# Model Configuration (Step 3)
model:
  encoder:
    model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    hidden_size: 384
    max_length: 512
  
  semantic_embedding:
    contrastive_learning:
      temperature: 0.1
      margin: 0.3
      negative_sampling_ratio: 3
  
  training_phases:
    - name: "contrastive_learning"
      epochs: 10
      learning_rate: 2e-5
      batch_size: 32

# Hyperparameter Optimization (Step 3)
optimization:
  n_trials: 50
  timeout: 3600  # 1 hour
  search_space:
    learning_rates: [1e-5, 2e-5, 5e-5, 1e-4]
    batch_sizes: [16, 32, 64]
    epochs: [5, 10, 15]
    temperatures: [0.05, 0.1, 0.2]
    margins: [0.1, 0.3, 0.5]
    negative_sampling_ratios: [2, 3, 5]

# Feature Extraction (Step 4)
feature_extraction:
  method: "knn"  # or "similarity_threshold"
  k_neighbors: 5
  similarity_threshold: 0.6
  min_occurrences: 2

# Model Training (Step 5)
models:
  multi_label_classifier:
    type: "random_forest"  # or "logistic_regression", "svm"
    max_depth: 10
    n_estimators: 100
  
  contrastive_learning:
    type: "orchestration"
    models_per_category: 4
    loss_type: "triplet_loss"
    margin: 0.3
    temperature: 0.1

# Evaluation (Step 6)
evaluation:
  metrics:
    classification: ["precision", "recall", "f1", "hamming_loss"]
    semantic: ["cosine_similarity", "semantic_similarity"]
    performance: ["training_time", "inference_time", "model_size"]
  
  visualization:
    comparison_charts: true
    per_category_analysis: true
    confusion_matrices: true

# Training Configuration
training:
  batch_size: 32
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  warmup_steps: 500
  weight_decay: 0.01
  early_stopping_patience: 5
  save_best_model: true
  checkpoint_dir: "experiments/checkpoints"
  log_dir: "experiments/logs"

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "experiments/logs/project.log"

# Hardware Configuration
hardware:
  device: "auto"  # "cpu", "cuda", "mps", or "auto"
  num_workers: 4
  pin_memory: true
  mixed_precision: true 