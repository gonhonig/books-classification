# Project Configuration
project:
  name: "books-classification"
  version: "1.0.0"
  description: "Multi-label classification of English book sentences using semantic embeddings"

# Data Configuration (Step 1)
data:
  corpus_name: "english_books_corpus"
  selected_books:
    - "Anna Karenina"
    - "The Adventures of Alice in Wonderland"
    - "Frankenstein"
    - "Wuthering Heights"
  max_sentences_per_book: null  # Use all sentences
  balanced_sentences_per_book: 5000  # Number of sentences to sample per book for balanced dataset
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  max_sentence_length: 512
  min_sentence_length: 10
  random_seed: 42
  
  # Data preprocessing
  preprocessing:
    remove_duplicates: true
    normalize_text: true
    remove_special_chars: false
    lowercase: false

# Semantic Model Selection (Step 2)
semantic_models:
  candidates:
    - "sentence-transformers/all-MiniLM-L6-v2"
    - "sentence-transformers/all-mpnet-base-v2"
    - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    - "sentence-transformers/paraphrase-MiniLM-L3-v2"
  
  # Selected best model from Step 2
  selected_model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  
  evaluation:
    similarity_threshold: 0.7
    test_pairs_file: "data/similarity_test_pairs.json"
    metrics: ["cosine_similarity", "semantic_similarity_score"]

# Model Configuration (Step 3)
model:
  encoder:
    model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    hidden_size: 384
    max_length: 512
  
  semantic_embedding:
    contrastive_learning:
      temperature: 0.1
      margin: 0.3
      negative_sampling_ratio: 3
  
  training_phases:
    - name: "contrastive_learning"
      epochs: 10
      learning_rate: 2e-5
      batch_size: 32

# Hyperparameter Optimization (Step 3)
optimization:
  # General optimization settings
  n_trials: 10
  cv_folds: 5
  timeout: 3600  # 1 hour total
  
  # Multi-label Classifier Optimization
  multi_label_classifier:
    timeout: 1800  # 30 minutes per model type
    
    # Random Forest parameters
    random_forest:
      n_estimators: [50, 100, 200, 300]
      max_depth: [3, 5, 10, 15, 20]
      min_samples_split: [2, 5, 10, 20]
      min_samples_leaf: [1, 2, 5, 10]
      max_features: ["sqrt", "log2", null]
      bootstrap: [True, False]
    
    # Logistic Regression parameters
    logistic_regression:
      C: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
      penalty: ["l1", "l2"]
      solver: ["liblinear", "saga"]
      max_iter: [500, 1000, 1500, 2000]
    
    # SVM parameters
    svm:
      C: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
      kernel: ["linear", "rbf", "poly"]
      gamma: ["scale", "auto"]

# Feature Extraction (Step 4) - Optimized Parameters
feature_extraction:
  method: "knn"  # or "similarity_threshold"
  k_neighbors: 15  # Optimized from hyperparameter optimization
  similarity_threshold: 0.95  # Optimized from hyperparameter optimization
  belonging_threshold: 0.3  # Optimized from hyperparameter optimization
  min_occurrences: 2

# Model Training (Step 5) - Optimized Parameters
models:
  multi_label_classifier:
    type: "random_forest"  # Best performing model (score: 0.9733)
    n_estimators: 179  # Optimized from hyperparameter optimization
    max_depth: 13  # Optimized from hyperparameter optimization
    min_samples_split: 2  # Optimized from hyperparameter optimization
    min_samples_leaf: 7  # Optimized from hyperparameter optimization
    max_features: null  # Optimized from hyperparameter optimization
    bootstrap: true  # Optimized from hyperparameter optimization
    
    # Optimization Results Summary:
    # - Random Forest: 0.9733 (BEST)
    # - Logistic Regression: 0.9538
    # - SVM: 0.9539
  
  # Optimized parameters for all models
  random_forest:
    n_estimators: 179
    max_depth: 13
    min_samples_split: 2
    min_samples_leaf: 7
    max_features: null
    bootstrap: true
    random_state: 42
  
  logistic_regression:
    C: 4.622589001020831
    penalty: "l1"
    solver: "saga"
    max_iter: 1287
    random_state: 42
  
  svm:
    C: 1.6409286730647923
    kernel: "poly"
    gamma: "scale"
    probability: true
    random_state: 42


# Evaluation (Step 6)
evaluation:
  metrics:
    classification: ["precision", "recall", "f1", "hamming_loss"]
    semantic: ["cosine_similarity", "semantic_similarity"]
    performance: ["training_time", "inference_time", "model_size"]
  
  visualization:
    comparison_charts: true
    per_category_analysis: true
    confusion_matrices: true

# Training Configuration
training:
  batch_size: 32
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  warmup_steps: 500
  weight_decay: 0.01
  early_stopping_patience: 5
  save_best_model: true
  checkpoint_dir: "experiments/checkpoints"
  log_dir: "experiments/logs"

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "experiments/logs/project.log"

# Hardware Configuration
hardware:
  device: "auto"  # "cpu", "cuda", "mps", or "auto"
  num_workers: 4
  pin_memory: true
  mixed_precision: true 